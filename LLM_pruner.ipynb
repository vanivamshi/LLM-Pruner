{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# gradual pruning\n",
        "# layer-wise pruning\n",
        "# Mixed Precision Training: Utilize PyTorchâ€™s AMP (Automatic Mixed Precision) to reduce memory usage and speed up training and inference without significant loss in accuracy.\n",
        "# Post-Training Quantization: Apply quantization techniques to the pruned model to further reduce the model size and computational requirements.\n",
        "# Distillation Post-Pruning: After pruning, apply knowledge distillation from the original model to the pruned model. This can help recover some of the lost accuracy due to pruning by aligning the outputs of the pruned model with those of the original.\n",
        "# Memory Mapping: If the dataset is large, use memory mapping techniques to avoid loading the entire dataset into memory, reducing the memory footprint.\n",
        "# Perplexity: Measures how well a probabilistic model predicts a sample of text"
      ],
      "metadata": {
        "id": "44Ja9xG2QPDD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKzaE9WWv8KW",
        "outputId": "83cc41e2-2760-4fa7-be91-408215577f02"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zuWgWBwI5LmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hi9B6Mxp5LpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "teacher_model = GPT2LMHeadModel.from_pretrained(model_name)  # Teacher model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the pruned model (student model)\n",
        "pruned_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "# Assume pruning has been performed on pruned_model\n",
        "\n",
        "# Add padding token to the tokenizer and resize model embeddings accordingly\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "    pruned_model.resize_token_embeddings(len(tokenizer))\n",
        "    teacher_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def gradual_layerwise_pruning(model, start_pruning_fractions, end_pruning_fractions, num_steps=3, sample_size=100000):\n",
        "    \"\"\"\n",
        "    Gradually increase the pruning fraction for each layer from start to end over a number of steps.\n",
        "    \"\"\"\n",
        "    for step in range(num_steps):\n",
        "        current_pruning_fractions = [\n",
        "            start + (end - start) * (step / num_steps)\n",
        "            for start, end in zip(start_pruning_fractions, end_pruning_fractions)\n",
        "        ]\n",
        "\n",
        "        # Apply pruning logic here if needed\n",
        "        # For now, this is a placeholder to show gradual changes\n",
        "        print(f\"Pruning step {step + 1}/{num_steps}: current pruning fractions {current_pruning_fractions}\")\n",
        "\n",
        "num_layers = sum(1 for _ in pruned_model.named_parameters() if 'weight' in _[0])\n",
        "start_pruning_fractions = [0.0] * num_layers\n",
        "end_pruning_fractions = [0.1, 0.15, 0.2] * (num_layers // 3) + [0.1] * (num_layers % 3)\n",
        "\n",
        "gradual_layerwise_pruning(pruned_model, start_pruning_fractions, end_pruning_fractions, num_steps=100)\n",
        "\n",
        "# Load dataset with streaming enabled\n",
        "dataset_name = 'wikitext'\n",
        "dataset_config = 'wikitext-103-raw-v1'\n",
        "dataset = load_dataset(dataset_name, dataset_config, split='train', streaming=True)\n",
        "\n",
        "# Estimate the number of examples and set batch size\n",
        "estimated_num_examples = 1000\n",
        "batch_size = 100\n",
        "\n",
        "# Prepare dataset function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Tokenize the dataset in chunks\n",
        "def preprocess_and_tokenize(dataset, tokenizer):\n",
        "    for batch in dataset:\n",
        "        yield tokenize_function({'text': batch['text']})\n",
        "\n",
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Calculate max_steps\n",
        "num_epochs = 1\n",
        "steps_per_epoch = estimated_num_examples // batch_size\n",
        "max_steps = num_epochs * steps_per_epoch\n",
        "\n",
        "# Define training arguments with Mixed Precision Training enabled\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    max_steps=max_steps,  # Specify the number of steps\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the pruned model (student model)\n",
        "trainer = Trainer(\n",
        "    model=pruned_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=preprocess_and_tokenize(dataset, tokenizer),\n",
        "    eval_dataset=preprocess_and_tokenize(dataset, tokenizer)\n",
        ")\n",
        "\n",
        "# Custom distillation training loop\n",
        "def distillation_loss(student_outputs, teacher_outputs, temperature=2.0):\n",
        "    \"\"\"\n",
        "    Compute distillation loss between student and teacher model outputs.\n",
        "    \"\"\"\n",
        "    student_logits = student_outputs.logits / temperature\n",
        "    teacher_logits = teacher_outputs.logits / temperature\n",
        "    return F.kl_div(F.log_softmax(student_logits, dim=-1), F.softmax(teacher_logits, dim=-1), reduction='batchmean')\n",
        "\n",
        "def distillation_train_loop(trainer, teacher_model, train_dataset, tokenizer, distillation_loss):\n",
        "    \"\"\"\n",
        "    Custom training loop for distillation.\n",
        "    \"\"\"\n",
        "    # Implement custom training logic here\n",
        "    pass\n",
        "\n",
        "# Run the custom distillation training loop\n",
        "distillation_train_loop(trainer, teacher_model, preprocess_and_tokenize(dataset, tokenizer), tokenizer, distillation_loss)\n",
        "\n",
        "# Save the fine-tuned student model\n",
        "model_save_path = './distilled-pruned-gpt2'\n",
        "pruned_model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Load the fine-tuned model for evaluation\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model=pruned_model, tokenizer=tokenizer)\n",
        "output = generator(\"Once upon a time\", max_length=50)\n",
        "print(output)\n",
        "\n",
        "# Post-Training Quantization\n",
        "# Create directory for saving quantized model\n",
        "quantized_model_save_path = './quantized-distilled-pruned-gpt2'\n",
        "os.makedirs(quantized_model_save_path, exist_ok=True)\n",
        "\n",
        "# Convert model to quantized version\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    pruned_model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save the quantized model\n",
        "torch.save(quantized_model.state_dict(), os.path.join(quantized_model_save_path, 'pytorch_model.bin'))\n",
        "\n",
        "# Load the quantized model for evaluation\n",
        "quantized_model.load_state_dict(torch.load(os.path.join(quantized_model_save_path, 'pytorch_model.bin')))\n",
        "generator_quantized = pipeline('text-generation', model=quantized_model, tokenizer=tokenizer)\n",
        "\n",
        "# Generate some text with the quantized model\n",
        "output_quantized = generator_quantized(\"Once upon a time\", max_length=50)\n",
        "print(output_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMG84RNw5LtX",
        "outputId": "10326401-5095-4b89-caa6-317a79d403f0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning step 1/100: current pruning fractions [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Pruning step 2/100: current pruning fractions [0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002, 0.001, 0.0015, 0.002]\n",
            "Pruning step 3/100: current pruning fractions [0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004, 0.002, 0.003, 0.004]\n",
            "Pruning step 4/100: current pruning fractions [0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006, 0.003, 0.0045, 0.006]\n",
            "Pruning step 5/100: current pruning fractions [0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008, 0.004, 0.006, 0.008]\n",
            "Pruning step 6/100: current pruning fractions [0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002, 0.005000000000000001, 0.0075, 0.010000000000000002]\n",
            "Pruning step 7/100: current pruning fractions [0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012, 0.006, 0.009, 0.012]\n",
            "Pruning step 8/100: current pruning fractions [0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002, 0.007000000000000001, 0.0105, 0.014000000000000002]\n",
            "Pruning step 9/100: current pruning fractions [0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016, 0.008, 0.012, 0.016]\n",
            "Pruning step 10/100: current pruning fractions [0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018, 0.009, 0.0135, 0.018]\n",
            "Pruning step 11/100: current pruning fractions [0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004, 0.010000000000000002, 0.015, 0.020000000000000004]\n",
            "Pruning step 12/100: current pruning fractions [0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002, 0.011000000000000001, 0.0165, 0.022000000000000002]\n",
            "Pruning step 13/100: current pruning fractions [0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024, 0.012, 0.018, 0.024]\n",
            "Pruning step 14/100: current pruning fractions [0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002, 0.013000000000000001, 0.0195, 0.026000000000000002]\n",
            "Pruning step 15/100: current pruning fractions [0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004, 0.014000000000000002, 0.021, 0.028000000000000004]\n",
            "Pruning step 16/100: current pruning fractions [0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03, 0.015, 0.0225, 0.03]\n",
            "Pruning step 17/100: current pruning fractions [0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032, 0.016, 0.024, 0.032]\n",
            "Pruning step 18/100: current pruning fractions [0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034, 0.017, 0.025500000000000002, 0.034]\n",
            "Pruning step 19/100: current pruning fractions [0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036, 0.018, 0.027, 0.036]\n",
            "Pruning step 20/100: current pruning fractions [0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006, 0.019000000000000003, 0.028499999999999998, 0.038000000000000006]\n",
            "Pruning step 21/100: current pruning fractions [0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001, 0.020000000000000004, 0.03, 0.04000000000000001]\n",
            "Pruning step 22/100: current pruning fractions [0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042, 0.021, 0.0315, 0.042]\n",
            "Pruning step 23/100: current pruning fractions [0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004, 0.022000000000000002, 0.033, 0.044000000000000004]\n",
            "Pruning step 24/100: current pruning fractions [0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006, 0.023000000000000003, 0.0345, 0.046000000000000006]\n",
            "Pruning step 25/100: current pruning fractions [0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048, 0.024, 0.036, 0.048]\n",
            "Pruning step 26/100: current pruning fractions [0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05, 0.025, 0.0375, 0.05]\n",
            "Pruning step 27/100: current pruning fractions [0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005, 0.026000000000000002, 0.039, 0.052000000000000005]\n",
            "Pruning step 28/100: current pruning fractions [0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006, 0.027000000000000003, 0.0405, 0.054000000000000006]\n",
            "Pruning step 29/100: current pruning fractions [0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001, 0.028000000000000004, 0.042, 0.05600000000000001]\n",
            "Pruning step 30/100: current pruning fractions [0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996, 0.028999999999999998, 0.0435, 0.057999999999999996]\n",
            "Pruning step 31/100: current pruning fractions [0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06, 0.03, 0.045, 0.06]\n",
            "Pruning step 32/100: current pruning fractions [0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062, 0.031, 0.0465, 0.062]\n",
            "Pruning step 33/100: current pruning fractions [0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064, 0.032, 0.048, 0.064]\n",
            "Pruning step 34/100: current pruning fractions [0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066, 0.033, 0.0495, 0.066]\n",
            "Pruning step 35/100: current pruning fractions [0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068, 0.034, 0.051000000000000004, 0.068]\n",
            "Pruning step 36/100: current pruning fractions [0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999, 0.034999999999999996, 0.0525, 0.06999999999999999]\n",
            "Pruning step 37/100: current pruning fractions [0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072, 0.036, 0.054, 0.072]\n",
            "Pruning step 38/100: current pruning fractions [0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074, 0.037, 0.0555, 0.074]\n",
            "Pruning step 39/100: current pruning fractions [0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001, 0.038000000000000006, 0.056999999999999995, 0.07600000000000001]\n",
            "Pruning step 40/100: current pruning fractions [0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001, 0.03900000000000001, 0.058499999999999996, 0.07800000000000001]\n",
            "Pruning step 41/100: current pruning fractions [0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002, 0.04000000000000001, 0.06, 0.08000000000000002]\n",
            "Pruning step 42/100: current pruning fractions [0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082, 0.041, 0.06149999999999999, 0.082]\n",
            "Pruning step 43/100: current pruning fractions [0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084, 0.042, 0.063, 0.084]\n",
            "Pruning step 44/100: current pruning fractions [0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001, 0.043000000000000003, 0.0645, 0.08600000000000001]\n",
            "Pruning step 45/100: current pruning fractions [0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001, 0.044000000000000004, 0.066, 0.08800000000000001]\n",
            "Pruning step 46/100: current pruning fractions [0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001, 0.045000000000000005, 0.0675, 0.09000000000000001]\n",
            "Pruning step 47/100: current pruning fractions [0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001, 0.046000000000000006, 0.069, 0.09200000000000001]\n",
            "Pruning step 48/100: current pruning fractions [0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094, 0.047, 0.0705, 0.094]\n",
            "Pruning step 49/100: current pruning fractions [0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096, 0.048, 0.072, 0.096]\n",
            "Pruning step 50/100: current pruning fractions [0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098, 0.049, 0.0735, 0.098]\n",
            "Pruning step 51/100: current pruning fractions [0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1, 0.05, 0.075, 0.1]\n",
            "Pruning step 52/100: current pruning fractions [0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001, 0.051000000000000004, 0.0765, 0.10200000000000001]\n",
            "Pruning step 53/100: current pruning fractions [0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001, 0.052000000000000005, 0.078, 0.10400000000000001]\n",
            "Pruning step 54/100: current pruning fractions [0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001, 0.053000000000000005, 0.0795, 0.10600000000000001]\n",
            "Pruning step 55/100: current pruning fractions [0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001, 0.054000000000000006, 0.081, 0.10800000000000001]\n",
            "Pruning step 56/100: current pruning fractions [0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001, 0.05500000000000001, 0.0825, 0.11000000000000001]\n",
            "Pruning step 57/100: current pruning fractions [0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002, 0.05600000000000001, 0.084, 0.11200000000000002]\n",
            "Pruning step 58/100: current pruning fractions [0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999, 0.056999999999999995, 0.08549999999999999, 0.11399999999999999]\n",
            "Pruning step 59/100: current pruning fractions [0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999, 0.057999999999999996, 0.087, 0.11599999999999999]\n",
            "Pruning step 60/100: current pruning fractions [0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118, 0.059, 0.0885, 0.118]\n",
            "Pruning step 61/100: current pruning fractions [0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12, 0.06, 0.09, 0.12]\n",
            "Pruning step 62/100: current pruning fractions [0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122, 0.061, 0.0915, 0.122]\n",
            "Pruning step 63/100: current pruning fractions [0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124, 0.062, 0.093, 0.124]\n",
            "Pruning step 64/100: current pruning fractions [0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126, 0.063, 0.0945, 0.126]\n",
            "Pruning step 65/100: current pruning fractions [0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128, 0.064, 0.096, 0.128]\n",
            "Pruning step 66/100: current pruning fractions [0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13, 0.065, 0.0975, 0.13]\n",
            "Pruning step 67/100: current pruning fractions [0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132, 0.066, 0.099, 0.132]\n",
            "Pruning step 68/100: current pruning fractions [0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134, 0.067, 0.1005, 0.134]\n",
            "Pruning step 69/100: current pruning fractions [0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136, 0.068, 0.10200000000000001, 0.136]\n",
            "Pruning step 70/100: current pruning fractions [0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998, 0.06899999999999999, 0.1035, 0.13799999999999998]\n",
            "Pruning step 71/100: current pruning fractions [0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999, 0.06999999999999999, 0.105, 0.13999999999999999]\n",
            "Pruning step 72/100: current pruning fractions [0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142, 0.071, 0.1065, 0.142]\n",
            "Pruning step 73/100: current pruning fractions [0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144, 0.072, 0.108, 0.144]\n",
            "Pruning step 74/100: current pruning fractions [0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146, 0.073, 0.1095, 0.146]\n",
            "Pruning step 75/100: current pruning fractions [0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148, 0.074, 0.111, 0.148]\n",
            "Pruning step 76/100: current pruning fractions [0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002, 0.07500000000000001, 0.11249999999999999, 0.15000000000000002]\n",
            "Pruning step 77/100: current pruning fractions [0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002, 0.07600000000000001, 0.11399999999999999, 0.15200000000000002]\n",
            "Pruning step 78/100: current pruning fractions [0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003, 0.07700000000000001, 0.11549999999999999, 0.15400000000000003]\n",
            "Pruning step 79/100: current pruning fractions [0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003, 0.07800000000000001, 0.11699999999999999, 0.15600000000000003]\n",
            "Pruning step 80/100: current pruning fractions [0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003, 0.07900000000000001, 0.1185, 0.15800000000000003]\n",
            "Pruning step 81/100: current pruning fractions [0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003, 0.08000000000000002, 0.12, 0.16000000000000003]\n",
            "Pruning step 82/100: current pruning fractions [0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003, 0.08100000000000002, 0.1215, 0.16200000000000003]\n",
            "Pruning step 83/100: current pruning fractions [0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164, 0.082, 0.12299999999999998, 0.164]\n",
            "Pruning step 84/100: current pruning fractions [0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166, 0.083, 0.12449999999999999, 0.166]\n",
            "Pruning step 85/100: current pruning fractions [0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168, 0.084, 0.126, 0.168]\n",
            "Pruning step 86/100: current pruning fractions [0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17, 0.085, 0.1275, 0.17]\n",
            "Pruning step 87/100: current pruning fractions [0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001, 0.08600000000000001, 0.129, 0.17200000000000001]\n",
            "Pruning step 88/100: current pruning fractions [0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002, 0.08700000000000001, 0.1305, 0.17400000000000002]\n",
            "Pruning step 89/100: current pruning fractions [0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002, 0.08800000000000001, 0.132, 0.17600000000000002]\n",
            "Pruning step 90/100: current pruning fractions [0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002, 0.08900000000000001, 0.1335, 0.17800000000000002]\n",
            "Pruning step 91/100: current pruning fractions [0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002, 0.09000000000000001, 0.135, 0.18000000000000002]\n",
            "Pruning step 92/100: current pruning fractions [0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002, 0.09100000000000001, 0.1365, 0.18200000000000002]\n",
            "Pruning step 93/100: current pruning fractions [0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002, 0.09200000000000001, 0.138, 0.18400000000000002]\n",
            "Pruning step 94/100: current pruning fractions [0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003, 0.09300000000000001, 0.1395, 0.18600000000000003]\n",
            "Pruning step 95/100: current pruning fractions [0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188, 0.094, 0.141, 0.188]\n",
            "Pruning step 96/100: current pruning fractions [0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19, 0.095, 0.1425, 0.19]\n",
            "Pruning step 97/100: current pruning fractions [0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192, 0.096, 0.144, 0.192]\n",
            "Pruning step 98/100: current pruning fractions [0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194, 0.097, 0.1455, 0.194]\n",
            "Pruning step 99/100: current pruning fractions [0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196, 0.098, 0.147, 0.196]\n",
            "Pruning step 100/100: current pruning fractions [0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198, 0.099, 0.1485, 0.198]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Once upon a time, what seems to you that the lightest person could make a living being with that power, you felt the power to control it.\"\\n\\n\\nA little later, with the arrival of the angel of death by his side, it'}]\n",
            "[{'generated_text': 'Once upon a time and the ancillh, that in time in in of, other\\n18\\n|- is as only in the thein, by with out, and are, this to in a a a and of; and afterwards as'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuzRKJ0ly4ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "import math\n",
        "\n",
        "# Define the model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the original and pruned models\n",
        "original_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "pruned_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set models to evaluation mode\n",
        "original_model.eval()\n",
        "pruned_model.eval()\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=50):\n",
        "    \"\"\"\n",
        "    Generate text using the model with a given prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The model to use for generation.\n",
        "        tokenizer: The tokenizer to use with the model.\n",
        "        prompt: The input text to base the generation on.\n",
        "        max_length: The maximum length of the generated text.\n",
        "\n",
        "    Returns:\n",
        "        Generated text.\n",
        "    \"\"\"\n",
        "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    result = generator(prompt, max_length=max_length)\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "def calculate_perplexity(model, tokenizer, prompt):\n",
        "    \"\"\"\n",
        "    Calculate perplexity of the model given a prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The model to evaluate.\n",
        "        tokenizer: The tokenizer to use with the model.\n",
        "        prompt: The input text to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        Perplexity score.\n",
        "    \"\"\"\n",
        "    encodings = tokenizer(prompt, return_tensors='pt')\n",
        "    input_ids = encodings.input_ids\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        perplexity = math.exp(loss.item())\n",
        "    return perplexity\n",
        "\n",
        "def run_text_generation_evaluation(model, tokenizer, prompts, max_length=50):\n",
        "    \"\"\"\n",
        "    Evaluate the model's text generation capabilities with a set of prompts.\n",
        "\n",
        "    Args:\n",
        "        model: The model to use for generation.\n",
        "        tokenizer: The tokenizer to use with the model.\n",
        "        prompts: List of prompts to generate text for.\n",
        "        max_length: The maximum length of the generated text.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prompts and their generated texts.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for prompt in prompts:\n",
        "        generated_text = generate_text(model, tokenizer, prompt, max_length)\n",
        "        perplexity = calculate_perplexity(model, tokenizer, prompt)\n",
        "        results[prompt] = {\n",
        "            'generated_text': generated_text,\n",
        "            'perplexity': perplexity\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Example prompts for evaluation\n",
        "prompts = [\n",
        "    \"Once upon a time in a distant land\",\n",
        "    \"In the near future, technology\",\n",
        "    \"The stock market surged today\",\n",
        "    \"Artificial intelligence is transforming\",\n",
        "]\n",
        "\n",
        "# Run evaluation for both original and pruned models\n",
        "print(\"Evaluating Original Model:\")\n",
        "generation_results_original = run_text_generation_evaluation(original_model, tokenizer, prompts, max_length=50)\n",
        "\n",
        "print(\"\\nEvaluating Pruned Model:\")\n",
        "generation_results_pruned = run_text_generation_evaluation(pruned_model, tokenizer, prompts, max_length=50)\n",
        "\n",
        "# Print results for original model\n",
        "print(\"\\nText Generation Results for Original Model:\")\n",
        "for prompt, result in generation_results_original.items():\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated Text: {result['generated_text']}\")\n",
        "    print(f\"Perplexity: {result['perplexity']}\")\n",
        "    print()\n",
        "\n",
        "# Print results for pruned model\n",
        "print(\"\\nText Generation Results for Pruned Model:\")\n",
        "for prompt, result in generation_results_pruned.items():\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated Text: {result['generated_text']}\")\n",
        "    print(f\"Perplexity: {result['perplexity']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42KuRA9jy393",
        "outputId": "c7800423-b783-4f66-bc4c-e0abfeb16bdf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Original Model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Pruned Model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text Generation Results for Original Model:\n",
            "Prompt: Once upon a time in a distant land\n",
            "Generated Text: Once upon a time in a distant land, the gods and goddesses were summoned together for a banquet in Aeneid to have the pleasure of drinking wine with their hosts. And the banquet began with the banquet of a little girl named Grecia\n",
            "Perplexity: 32.54282968895235\n",
            "\n",
            "Prompt: In the near future, technology\n",
            "Generated Text: In the near future, technology for a humanized face might be needed for the next generation of augmented reality. One way of doing this would be to make the face as beautiful as possible as well as make the eyes as human-like, such that\n",
            "Perplexity: 21.24105993908553\n",
            "\n",
            "Prompt: The stock market surged today\n",
            "Generated Text: The stock market surged today after the Fed's most recent report released on Friday. The value of the benchmark 10-year Treasury bond, first pegged to the Federal Reserve, rose by $5.9 billion in early trading.\n",
            "\n",
            "The increase coincides\n",
            "Perplexity: 278.4601176889584\n",
            "\n",
            "Prompt: Artificial intelligence is transforming\n",
            "Generated Text: Artificial intelligence is transforming our lives. Here is a new example.\"\n",
            "\n",
            "We're talking about how much computing power we can store on our computers, how fast we can drive us, how cheap we can use them, how much computing power we\n",
            "Perplexity: 162.36456804117896\n",
            "\n",
            "\n",
            "Text Generation Results for Pruned Model:\n",
            "Prompt: Once upon a time in a distant land\n",
            "Generated Text: Once upon a time in a distant land they did not exist. But when the universe has once again come to them (from their own past), it will surely prove much more amazing. And I will go down in history as the greatest physicist; for\n",
            "Perplexity: 32.54282968895235\n",
            "\n",
            "Prompt: In the near future, technology\n",
            "Generated Text: In the near future, technology will also enable players to play a large number of games in one place online, so this can make it easier for players to do the same thing in a small amount of time.\n",
            "\n",
            "Some experts think the most promising\n",
            "Perplexity: 21.24105993908553\n",
            "\n",
            "Prompt: The stock market surged today\n",
            "Generated Text: The stock market surged today, as investors bought up the share price in anticipation of some sort of dramatic change and demand for its second largest trade.\n",
            "\n",
            "The stock market could collapse, so let's focus on how the market looks during a rally.\n",
            "Perplexity: 278.4601176889584\n",
            "\n",
            "Prompt: Artificial intelligence is transforming\n",
            "Generated Text: Artificial intelligence is transforming our lives and our workplace by a wide margin. But not every decision we make about how we use computers and our phones, computers and our TVs has a major impact on the lives of others. AI may not even be the\n",
            "Perplexity: 162.36456804117896\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qE3pR_Du_WF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model inference time\n",
        "\n",
        "import time\n",
        "\n",
        "def measure_inference_time(generator, prompts, max_length=50):\n",
        "    \"\"\"\n",
        "    Measure the average inference time for generating text with given prompts.\n",
        "\n",
        "    Args:\n",
        "        generator: The pipeline used for text generation.\n",
        "        prompts: List of prompts to generate text for.\n",
        "        max_length: The maximum length of the generated text.\n",
        "\n",
        "    Returns:\n",
        "        Average inference time in seconds.\n",
        "    \"\"\"\n",
        "    total_time = 0\n",
        "    for prompt in prompts:\n",
        "        start_time = time.time()\n",
        "        _ = generator(prompt, max_length=max_length)\n",
        "        end_time = time.time()\n",
        "        total_time += (end_time - start_time)\n",
        "    average_time = total_time / len(prompts)\n",
        "    return average_time\n",
        "\n",
        "# Define the model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the original model\n",
        "original_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "original_model.eval()\n",
        "original_generator = pipeline(\"text-generation\", model=original_model, tokenizer=tokenizer)\n",
        "\n",
        "# Measure inference time for the original model\n",
        "original_inference_time = measure_inference_time(original_generator, prompts)\n",
        "print(f\"Average Inference Time for Original Model: {original_inference_time:.4f} seconds\")\n",
        "\n",
        "# Load the pruned model\n",
        "pruned_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "pruned_model.eval()\n",
        "pruned_generator = pipeline(\"text-generation\", model=pruned_model, tokenizer=tokenizer)\n",
        "\n",
        "# Measure inference time for the pruned model\n",
        "pruned_inference_time = measure_inference_time(pruned_generator, prompts)\n",
        "print(f\"Average Inference Time for Pruned Model: {pruned_inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwyy1Pfc_WTd",
        "outputId": "d3fe72f0-f696-4259-9d2b-1bfc70d2e892"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Inference Time for Original Model: 2.8854 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Inference Time for Pruned Model: 2.9150 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oUNxL7Si_Zwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory usage\n",
        "\n",
        "import tracemalloc\n",
        "\n",
        "def measure_memory_usage_tracemalloc(generator, prompts, max_length=50):\n",
        "    \"\"\"\n",
        "    Measure the memory usage during text generation using tracemalloc.\n",
        "\n",
        "    Args:\n",
        "        generator: The pipeline used for text generation.\n",
        "        prompts: List of prompts to generate text for.\n",
        "        max_length: The maximum length of the generated text.\n",
        "\n",
        "    Returns:\n",
        "        Maximum memory usage in MB.\n",
        "    \"\"\"\n",
        "    tracemalloc.start()\n",
        "\n",
        "    # Generate text\n",
        "    for prompt in prompts:\n",
        "        _ = generator(prompt, max_length=max_length)\n",
        "\n",
        "    snapshot = tracemalloc.take_snapshot()\n",
        "    top_stats = snapshot.statistics('lineno')\n",
        "\n",
        "    print(\"Top 10 lines with highest memory usage:\")\n",
        "    for stat in top_stats[:10]:\n",
        "        print(stat)\n",
        "\n",
        "    # Calculate memory usage\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    return peak / 1024 / 1024  # Convert to MB\n",
        "\n",
        "# Measure memory usage for the original model\n",
        "original_memory_usage_tracemalloc = measure_memory_usage_tracemalloc(original_generator, prompts)\n",
        "print(f\"Memory Usage for Original Model: {original_memory_usage_tracemalloc:.2f} MB\")\n",
        "\n",
        "# Measure memory usage for the pruned model\n",
        "pruned_memory_usage_tracemalloc = measure_memory_usage_tracemalloc(pruned_generator, prompts)\n",
        "print(f\"Memory Usage for Pruned Model: {pruned_memory_usage_tracemalloc:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt6TUuvu_kj0",
        "outputId": "5d9ff1d3-ba8a-41bb-a2c4-c0a02ddbbb6b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 lines with highest memory usage:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541: size=20.9 KiB, count=230, average=93 B\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2573: size=15.0 KiB, count=175, average=88 B\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: size=7824 B, count=49, average=160 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:634: size=6136 B, count=104, average=59 B\n",
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:28: size=4632 B, count=1, average=4632 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1322: size=4256 B, count=66, average=64 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:633: size=3363 B, count=57, average=59 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:677: size=2183 B, count=37, average=59 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2702: size=1770 B, count=30, average=59 B\n",
            "/usr/lib/python3.10/json/encoder.py:253: size=1760 B, count=44, average=40 B\n",
            "Memory Usage for Original Model: 0.21 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 lines with highest memory usage:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541: size=19.3 KiB, count=210, average=94 B\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2573: size=15.1 KiB, count=176, average=88 B\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: size=7520 B, count=45, average=167 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:634: size=5192 B, count=88, average=59 B\n",
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:28: size=4632 B, count=1, average=4632 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1322: size=3039 B, count=47, average=65 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:633: size=2124 B, count=36, average=59 B\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2702: size=1829 B, count=31, average=59 B\n",
            "/usr/lib/python3.10/json/encoder.py:253: size=1760 B, count=44, average=40 B\n",
            "/usr/local/lib/python3.10/dist-packages/zmq/sugar/attrsettr.py:44: size=1595 B, count=29, average=55 B\n",
            "Memory Usage for Pruned Model: 0.19 MB\n"
          ]
        }
      ]
    }
  ]
}